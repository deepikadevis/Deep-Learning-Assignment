{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def28f0e",
   "metadata": {},
   "source": [
    "# Assignment_6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742e3f7",
   "metadata": {},
   "source": [
    "## 1.\tWhat are the advantages of a CNN over a fully connected DNN for image classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606f005",
   "metadata": {},
   "source": [
    "These are the main advantages of a CNN over a fully connected\n",
    "DNN for image classification:\n",
    "\n",
    "Because consecutive layers are only partially connected\n",
    "and because it heavily reuses its weights, a CNN has\n",
    "many fewer parameters than a fully connected DNN,\n",
    "which makes it much faster to train, reduces the risk of\n",
    "overfitting, and requires much less training data.\n",
    "\n",
    "When a CNN has learned a kernel that can detect a\n",
    "particular feature, it can detect that feature anywhere in\n",
    "the image. In contrast, when a DNN learns a feature in\n",
    "one location, it can detect it only in that particular\n",
    "location. Since images typically have very repetitive\n",
    "features, CNNs are able to generalize much better than\n",
    "DNNs for image processing tasks such as classification,\n",
    "using fewer training examples.\n",
    "\n",
    "Finally, a DNN has no prior knowledge of how pixels are\n",
    "organized; it does not know that nearby pixels are close.\n",
    "A CNN’s architecture embeds this prior knowledge.\n",
    "Lower layers typically identify features in small areas of\n",
    "the images, while higher layers combine the lower-level\n",
    "features into larger features. This works well with most\n",
    "natural images, giving CNNs a decisive head start\n",
    "compared to DNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6562f6ac",
   "metadata": {},
   "source": [
    "## 2.\tConsider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "\n",
    "## What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96326a8",
   "metadata": {},
   "source": [
    "Let’s compute how many parameters the CNN has. Since its first\n",
    "convolutional layer has 3 × 3 kernels, and the input has three\n",
    "channels (red, green, and blue), each feature map has 3 × 3 × 3\n",
    "weights, plus a bias term. That’s 28 parameters per feature map.\n",
    "Since this first convolutional layer has 100 feature maps, it has a\n",
    "total of 2,800 parameters. The second convolutional layer has 3 ×\n",
    "3 kernels and its input is the set of 100 feature maps of the\n",
    "previous layer, so each feature map has 3 × 3 × 100 = 900\n",
    "weights, plus a bias term. Since it has 200 feature maps, this layer\n",
    "has 901 × 200 = 180,200 parameters. Finally, the third and last\n",
    "convolutional layer also has 3 × 3 kernels, and its input is the set\n",
    "of 200 feature maps of the previous layers, so each feature map\n",
    "has 3 × 3 × 200 = 1,800 weights, plus a bias term. Since it has 400\n",
    "feature maps, this layer has a total of 1,801 × 400 = 720,400\n",
    "parameters. All in all, the CNN has 2,800 + 180,200 + 720,400 =\n",
    "903,400 parameters.\n",
    "\n",
    "Now let’s compute how much RAM this neural network will\n",
    "require (at least) when making a prediction for a single instance.\n",
    "First let’s compute the feature map size for each layer. Since we\n",
    "are using a stride of 2 and \"same\" padding, the horizontal and\n",
    "vertical dimensions of the feature maps are divided by 2 at each\n",
    "layer (rounding up if necessary). So, as the input channels are 200\n",
    "× 300 pixels, the first layer’s feature maps are 100 × 150, the\n",
    "second layer’s feature maps are 50 × 75, and the third layer’s\n",
    "feature maps are 25 × 38. Since 32 bits is 4 bytes and the first\n",
    "convolutional layer has 100 feature maps, this first layer takes up\n",
    "4 × 100 × 150 × 100 = 6 million bytes (6 MB). The second layer\n",
    "takes up 4 × 50 × 75 × 200 = 3 million bytes (3 MB). Finally, the\n",
    "third layer takes up 4 × 25 × 38 × 400 = 1,520,000 bytes (about\n",
    "1.5 MB). However, once a layer has been computed, the memory\n",
    "occupied by the previous layer can be released, so if everything is\n",
    "well optimized, only 6 + 3 = 9 million bytes (9 MB) of RAM will\n",
    "be required (when the second layer has just been computed, but\n",
    "the memory occupied by the first layer has not been released yet).\n",
    "But wait, you also need to add the memory occupied by the CNN’s\n",
    "parameters! We computed earlier that it has 903,400 parameters,\n",
    "each using up 4 bytes, so this adds 3,613,600 bytes (about 3.6\n",
    "MB). The total RAM required is therefore (at least) 12,613,600\n",
    "bytes (about 12.6 MB).\n",
    "\n",
    "Lastly, let’s compute the minimum amount of RAM required\n",
    "when training the CNN on a mini-batch of 50 images. During\n",
    "training TensorFlow uses backpropagation, which requires\n",
    "keeping all values computed during the forward pass until the\n",
    "reverse pass begins. So we must compute the total RAM required\n",
    "by all layers for a single instance and multiply that by 50. At this\n",
    "point, let’s start counting in megabytes rather than bytes. We\n",
    "computed before that the three layers require respectively 6, 3,\n",
    "and 1.5 MB for each instance. That’s a total of 10.5 MB per\n",
    "instance, so for 50 instances the total RAM required is 525 MB.\n",
    "Add to that the RAM required by the input images, which is 50 ×\n",
    "4 × 200 × 300 × 3 = 36 million bytes (36 MB), plus the RAM\n",
    "required for the model parameters, which is about 3.6 MB\n",
    "(computed earlier), plus some RAM for the gradients (we will\n",
    "neglect this since it can be released gradually as backpropagation\n",
    "goes down the layers during the reverse pass). We are up to a total\n",
    "of roughly 525 + 36 + 3.6 = 564.6 MB, and that’s really an\n",
    "optimistic bare minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c00c76",
   "metadata": {},
   "source": [
    "# 3.\tIf your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff555b1c",
   "metadata": {},
   "source": [
    " If your GPU runs out of memory while training a CNN, here are\n",
    "five things you could try to solve the problem (other than\n",
    "purchasing a GPU with more RAM):\n",
    "\n",
    "Reduce the mini-batch size.\n",
    "\n",
    "Reduce dimensionality using a larger stride in one or\n",
    "more layers.\n",
    "\n",
    "Remove one or more layers.\n",
    "\n",
    "Use 16-bit floats instead of 32-bit floats.\n",
    "\n",
    "Distribute the CNN across multiple devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef5272",
   "metadata": {},
   "source": [
    "# 4.\tWhy would you want to add a max pooling layer rather than a convolutional layer with the same stride?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c28177",
   "metadata": {},
   "source": [
    "A max pooling layer has no parameters at all, whereas a\n",
    "convolutional layer has quite a few"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07dee9",
   "metadata": {},
   "source": [
    "# 5.\tWhen would you want to add a local response normalization layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4083fe70",
   "metadata": {},
   "source": [
    "A local response normalization layer makes the neurons that most\n",
    "strongly activate inhibit neurons at the same location but in\n",
    "neighboring feature maps, which encourages different feature\n",
    "maps to specialize and pushes them apart, forcing them to explore\n",
    "a wider range of features. It is typically used in the lower layers to\n",
    "have a larger pool of low-level features that the upper layers can\n",
    "build upon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35dc869",
   "metadata": {},
   "source": [
    "# 6.\tCan you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e348d14",
   "metadata": {},
   "source": [
    "A local response normalization layer makes the neurons that most\n",
    "strongly activate inhibit neurons at the same location but in\n",
    "neighboring feature maps, which encourages different feature\n",
    "maps to specialize and pushes them apart, forcing them to explore\n",
    "a wider range of features. It is typically used in the lower layers to\n",
    "have a larger pool of low-level features that the upper layers can\n",
    "build upon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed72ab",
   "metadata": {},
   "source": [
    "# 7.\tWhat is a fully convolutional network? How can you convert a dense layer into a convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898d57f",
   "metadata": {},
   "source": [
    "Fully convolutional networks are neural networks composed\n",
    "exclusively of convolutional and pooling layers. FCNs can\n",
    "efficiently process images of any width and height (at least above\n",
    "the minimum size). They are most useful for object detection and\n",
    "semantic segmentation because they only need to look at the\n",
    "image once (instead of having to run a CNN multiple times on\n",
    "different parts of the image). If you have a CNN with some dense\n",
    "layers on top, you can convert these dense layers to convolutional\n",
    "layers to create an FCN: just replace the lowest dense layer with a\n",
    "convolutional layer with a kernel size equal to the layer’s input\n",
    "size, with one filter per neuron in the dense layer, and using\n",
    "\"valid\" padding. Generally the stride should be 1, but you can\n",
    "set it to a higher value if you want. The activation function should\n",
    "be the same as the dense layer’s. The other dense layers should be\n",
    "converted the same way, but using 1 × 1 filters. It is actually\n",
    "possible to convert a trained CNN this way by appropriately\n",
    "reshaping the dense layers’ weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39c009",
   "metadata": {},
   "source": [
    "# 8.\tWhat is the main technical difficulty of semantic segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea160f",
   "metadata": {},
   "source": [
    " The main technical difficulty of semantic segmentation is the fact\n",
    "that a lot of the spatial information gets lost in a CNN as the\n",
    "signal flows through each layer, especially in pooling layers and\n",
    "layers with a stride greater than 1. This spatial information needs\n",
    "to be restored somehow to accurately predict the class of each\n",
    "pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0f6c4",
   "metadata": {},
   "source": [
    "# 9.\tBuild your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6ecb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fb6700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 8s 1us/step\n",
      "11501568/11490434 [==============================] - 8s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full / 255.\n",
    "X_test = X_test / 255.\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99338c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 328s 162ms/step - loss: 0.2002 - accuracy: 0.9393 - val_loss: 0.0455 - val_accuracy: 0.9888\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 291s 169ms/step - loss: 0.0818 - accuracy: 0.9754 - val_loss: 0.0387 - val_accuracy: 0.9898\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 297s 173ms/step - loss: 0.0619 - accuracy: 0.9810 - val_loss: 0.0369 - val_accuracy: 0.9896\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 280s 163ms/step - loss: 0.0528 - accuracy: 0.9837 - val_loss: 0.0367 - val_accuracy: 0.9902\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 287s 167ms/step - loss: 0.0422 - accuracy: 0.9866 - val_loss: 0.0355 - val_accuracy: 0.9912\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 326s 190ms/step - loss: 0.0359 - accuracy: 0.9889 - val_loss: 0.0402 - val_accuracy: 0.9904\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 263s 153ms/step - loss: 0.0337 - accuracy: 0.9890 - val_loss: 0.0302 - val_accuracy: 0.9932\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 264s 154ms/step - loss: 0.0290 - accuracy: 0.9907 - val_loss: 0.0393 - val_accuracy: 0.9920\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 251s 146ms/step - loss: 0.0275 - accuracy: 0.9911 - val_loss: 0.0387 - val_accuracy: 0.9912\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 250s 145ms/step - loss: 0.0240 - accuracy: 0.9925 - val_loss: 0.0418 - val_accuracy: 0.9918\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0291 - accuracy: 0.9919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.029122035950422287, 0.9919000267982483]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb6ca02",
   "metadata": {},
   "source": [
    "# 10.\tUse transfer learning for large image classification, going through these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58d321",
   "metadata": {},
   "source": [
    "a.\tCreate a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9154f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "FLOWERS_URL = \"http://download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "FLOWERS_PATH = os.path.join(\"datasets\", \"flowers\")\n",
    "\n",
    "def fetch_flowers(url=FLOWERS_URL, path=FLOWERS_PATH):\n",
    "    if os.path.exists(FLOWERS_PATH):\n",
    "        return\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    tgz_path = os.path.join(path, \"flower_photos.tgz\")\n",
    "    urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n",
    "    flowers_tgz = tarfile.open(tgz_path)\n",
    "    flowers_tgz.extractall(path=path)\n",
    "    flowers_tgz.close()\n",
    "    os.remove(tgz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaf8fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_flowers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_root_path = os.path.join(FLOWERS_PATH, \"flower_photos\")\n",
    "flower_classes = sorted([dirname for dirname in os.listdir(flowers_root_path)\n",
    "                  if os.path.isdir(os.path.join(flowers_root_path, dirname))])\n",
    "flower_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8116cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "image_paths = defaultdict(list)\n",
    "\n",
    "for flower_class in flower_classes:\n",
    "    image_dir = os.path.join(flowers_root_path, flower_class)\n",
    "    for filepath in os.listdir(image_dir):\n",
    "        if filepath.endswith(\".jpg\"):\n",
    "            image_paths[flower_class].append(os.path.join(image_dir, filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paths in image_paths.values():\n",
    "    paths.sort()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a605a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "n_examples_per_class = 2\n",
    "\n",
    "for flower_class in flower_classes:\n",
    "    print(\"Class:\", flower_class)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for index, example_image_path in enumerate(image_paths[flower_class][:n_examples_per_class]):\n",
    "        example_image = mpimg.imread(example_image_path)[:, :, :channels]\n",
    "        plt.subplot(100 + n_examples_per_class * 10 + index + 1)\n",
    "        plt.title(\"{}x{}\".format(example_image.shape[1], example_image.shape[0]))\n",
    "        plt.imshow(example_image)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c2766",
   "metadata": {},
   "source": [
    "b.\tSplit it into a training set, a validation set, and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae842ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "info.splits[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = info.features[\"label\"].names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65938ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae71023",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae97f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e53ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "index = 0\n",
    "for image, label in train_set_raw.take(9):\n",
    "    index += 1\n",
    "    plt.subplot(3, 3, index)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Class: {}\".format(class_names[label]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8a8f7",
   "metadata": {},
   "source": [
    "c.\tBuild the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7176d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e26d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
    "    top_crop = (shape[0] - min_dim) // 4\n",
    "    bottom_crop = shape[0] - top_crop\n",
    "    left_crop = (shape[1] - min_dim) // 4\n",
    "    right_crop = shape[1] - left_crop\n",
    "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
    "\n",
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n",
    "\n",
    "def preprocess(image, label, randomize=False):\n",
    "    if randomize:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = central_crop(image)\n",
    "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label\n",
    "\n",
    "batch_size = 32\n",
    "train_set = train_set_raw.shuffle(1000).repeat()\n",
    "train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch, y_batch in train_set.take(1):\n",
    "    for index in range(9):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index] / 2 + 0.5)\n",
    "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch, y_batch in test_set.take(1):\n",
    "    for index in range(9):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index] / 2 + 0.5)\n",
    "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bc1a1",
   "metadata": {},
   "source": [
    "d.\tFine-tune a pretrained model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, layer in enumerate(base_model.layers):\n",
    "    print(index, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a772c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set,\n",
    "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9,\n",
    "                                 nesterov=True, decay=0.001)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set,\n",
    "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
    "                    epochs=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
