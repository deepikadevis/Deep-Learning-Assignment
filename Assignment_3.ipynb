{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa7c4b8",
   "metadata": {},
   "source": [
    "## 1.\tIs it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee088551",
   "metadata": {},
   "source": [
    "No, all weights should be sampled independently; they should not\n",
    "all have the same initial value. One important goal of sampling\n",
    "weights randomly is to break symmetry: if all the weights have\n",
    "the same initial value, even if that value is not zero, then\n",
    "symmetry is not broken (i.e., all neurons in a given layer are\n",
    "equivalent), and backpropagation will be unable to break it.\n",
    "Concretely, this means that all the neurons in any given layer will\n",
    "always have the same weights. It’s like having just one neuron per\n",
    "layer, and much slower. It is virtually impossible for such a\n",
    "configuration to converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea56979",
   "metadata": {},
   "source": [
    "## 2.\tIs it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb9e15",
   "metadata": {},
   "source": [
    "No, all weights should be sampled independently; they should not\n",
    "all have the same initial value. One important goal of sampling\n",
    "weights randomly is to break symmetry: if all the weights have\n",
    "the same initial value, even if that value is not zero, then\n",
    "symmetry is not broken (i.e., all neurons in a given layer are\n",
    "equivalent), and backpropagation will be unable to break it.\n",
    "Concretely, this means that all the neurons in any given layer will\n",
    "always have the same weights. It’s like having just one neuron per\n",
    "layer, and much slower. It is virtually impossible for such a\n",
    "configuration to converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80f3b6",
   "metadata": {},
   "source": [
    "## 3.\tName three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3c035",
   "metadata": {},
   "source": [
    "A few advantages of the SELU function over the ReLU function\n",
    "are:\n",
    "\n",
    "It can take on negative values, so the average output of\n",
    "the neurons in any given layer is typically closer to zero\n",
    "than when using the ReLU activation function (which\n",
    "never outputs negative values). This helps alleviate the\n",
    "vanishing gradients problem.\n",
    "\n",
    "It always has a nonzero derivative, which avoids the\n",
    "dying units issue that can affect ReLU units.\n",
    "\n",
    "When the conditions are right (i.e., if the model is\n",
    "sequential, and the weights are initialized using LeCun\n",
    "initialization, and the inputs are standardized, and there’s\n",
    "no incompatible layer or regularization, such as dropout\n",
    "or ℓ regularization), then the SELU activation function\n",
    "ensures the model is self-normalized, which solves the\n",
    "exploding/vanishing gradients problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d14ef8",
   "metadata": {},
   "source": [
    "## 4.\tIn which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0da85b",
   "metadata": {},
   "source": [
    "The SELU activation function is a good default. If you need the\n",
    "neural network to be as fast as possible, you can use one of the\n",
    "leaky ReLU variants instead (e.g., a simple leaky ReLU using the\n",
    "default hyperparameter value). The simplicity of the ReLU\n",
    "activation function makes it many people’s preferred option,\n",
    "despite the fact that it is generally outperformed by SELU and\n",
    "leaky ReLU. However, the ReLU activation function’s ability to\n",
    "output precisely zero can be useful in some cases. Moreover, it can sometimes benefit from optimized\n",
    "implementation as well as from hardware acceleration. The\n",
    "hyperbolic tangent (tanh) can be useful in the output layer if you\n",
    "need to output a number between –1 and 1, but nowadays it is not\n",
    "used much in hidden layers (except in recurrent nets). The logistic\n",
    "activation function is also useful in the output layer when you\n",
    "need to estimate a probability (e.g., for binary classification), but\n",
    "is rarely used in hidden layers (there are exceptions—for\n",
    "example, for the coding layer of variational autoencoders. Finally, the softmax activation function is useful in\n",
    "the output layer to output probabilities for mutually exclusive\n",
    "classes, but it is rarely (if ever) used in hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0c400",
   "metadata": {},
   "source": [
    "## 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e20be",
   "metadata": {},
   "source": [
    "If you set the momentum hyperparameter too close to 1 (e.g.,\n",
    "0.99999) when using an SGD optimizer, then the algorithm will\n",
    "likely pick up a lot of speed, hopefully moving roughly toward the\n",
    "global minimum, but its momentum will carry it right past the\n",
    "minimum. Then it will slow down and come back, accelerate\n",
    "again, overshoot again, and so on. It may oscillate this way many\n",
    "times before converging, so overall it will take much longer to\n",
    "converge than with a smaller momentum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52e5e7",
   "metadata": {},
   "source": [
    "## 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04dab35",
   "metadata": {},
   "source": [
    "One way to produce a sparse model (i.e., with most weights equal\n",
    "to zero) is to train the model normally, then zero out tiny weights.\n",
    "For more sparsity, you can apply ℓ regularization during training,\n",
    "which pushes the optimizer toward sparsity. A third option is to\n",
    "use the TensorFlow Model Optimization Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a649b51",
   "metadata": {},
   "source": [
    "## 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50abfd",
   "metadata": {},
   "source": [
    "Yes, dropout does slow down training, in general roughly by a\n",
    "factor of two. However, it has no impact on inference speed since\n",
    "it is only turned on during training. MC Dropout is exactly like\n",
    "dropout during training, but it is still active during inference, so\n",
    "each inference is slowed down slightly. More importantly, when\n",
    "using MC Dropout you generally want to run inference 10 times\n",
    "or more to get better predictions. This means that making\n",
    "predictions is slowed down by a factor of 10 or more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65864e0",
   "metadata": {},
   "source": [
    "## 8.\tPractice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474e7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2bcb58",
   "metadata": {},
   "source": [
    "## a: Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e4dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1f50c",
   "metadata": {},
   "source": [
    "## b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_ data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a0dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c204fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e8d1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08876cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62721df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9544), started 1:03:17 ago. (Use '!kill 9544' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1f1d325958442748\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1f1d325958442748\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f75b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 49s 22ms/step - loss: 4.0822 - accuracy: 0.1705 - val_loss: 2.1179 - val_accuracy: 0.2326\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 2.0592 - accuracy: 0.2462 - val_loss: 2.1213 - val_accuracy: 0.2340\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.9363 - accuracy: 0.2911 - val_loss: 2.0175 - val_accuracy: 0.2702\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.8588 - accuracy: 0.3209 - val_loss: 1.8617 - val_accuracy: 0.3344\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.7975 - accuracy: 0.3451 - val_loss: 1.8554 - val_accuracy: 0.3308\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 1.7525 - accuracy: 0.3660 - val_loss: 1.7481 - val_accuracy: 0.3714\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 1.7116 - accuracy: 0.3770 - val_loss: 1.7146 - val_accuracy: 0.3698\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.6762 - accuracy: 0.3926 - val_loss: 1.6698 - val_accuracy: 0.3980\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.6465 - accuracy: 0.4025 - val_loss: 1.6616 - val_accuracy: 0.3996\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.6190 - accuracy: 0.4171 - val_loss: 1.6855 - val_accuracy: 0.3894\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.5971 - accuracy: 0.4239 - val_loss: 1.6601 - val_accuracy: 0.3952\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.5778 - accuracy: 0.4307 - val_loss: 1.6455 - val_accuracy: 0.4108\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.5579 - accuracy: 0.4389 - val_loss: 1.6648 - val_accuracy: 0.3968\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.5400 - accuracy: 0.4455 - val_loss: 1.5893 - val_accuracy: 0.4168\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 1.5242 - accuracy: 0.4526 - val_loss: 1.5928 - val_accuracy: 0.4246\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.5061 - accuracy: 0.4597 - val_loss: 1.5684 - val_accuracy: 0.4380\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.4954 - accuracy: 0.4645 - val_loss: 1.5530 - val_accuracy: 0.4476\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.4787 - accuracy: 0.4678 - val_loss: 1.5748 - val_accuracy: 0.4294 1.4800 - \n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.4638 - accuracy: 0.4752 - val_loss: 1.5583 - val_accuracy: 0.4412\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4532 - accuracy: 0.4769 - val_loss: 1.5660 - val_accuracy: 0.4364oss: 1.4518 - ac - ETA: 0s - loss:\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.4413 - accuracy: 0.4827 - val_loss: 1.5752 - val_accuracy: 0.4356\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.4305 - accuracy: 0.4853 - val_loss: 1.5342 - val_accuracy: 0.4460\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 1.4176 - accuracy: 0.4942 - val_loss: 1.5334 - val_accuracy: 0.4528\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4057 - accuracy: 0.4949 - val_loss: 1.5535 - val_accuracy: 0.4442\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3920 - accuracy: 0.5012 - val_loss: 1.5238 - val_accuracy: 0.4476\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3821 - accuracy: 0.5039 - val_loss: 1.5621 - val_accuracy: 0.4418\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3717 - accuracy: 0.5071 - val_loss: 1.5321 - val_accuracy: 0.4542\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3620 - accuracy: 0.5120 - val_loss: 1.5263 - val_accuracy: 0.4576\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3547 - accuracy: 0.5099 - val_loss: 1.5484 - val_accuracy: 0.4512\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3433 - accuracy: 0.5170 - val_loss: 1.5758 - val_accuracy: 0.4480\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3355 - accuracy: 0.5186 - val_loss: 1.5404 - val_accuracy: 0.4564\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3269 - accuracy: 0.5221 - val_loss: 1.5303 - val_accuracy: 0.4720\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3173 - accuracy: 0.5269 - val_loss: 1.5317 - val_accuracy: 0.4692\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3074 - accuracy: 0.5311 - val_loss: 1.5502 - val_accuracy: 0.4518\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3013 - accuracy: 0.5307 - val_loss: 1.5544 - val_accuracy: 0.4610\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.2919 - accuracy: 0.5341 - val_loss: 1.5602 - val_accuracy: 0.4616\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2813 - accuracy: 0.5410 - val_loss: 1.5223 - val_accuracy: 0.4720\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.2768 - accuracy: 0.5401 - val_loss: 1.5510 - val_accuracy: 0.4666\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.2658 - accuracy: 0.5427 - val_loss: 1.5549 - val_accuracy: 0.4506\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2577 - accuracy: 0.5493 - val_loss: 1.5455 - val_accuracy: 0.4618\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2471 - accuracy: 0.5500 - val_loss: 1.5495 - val_accuracy: 0.4628\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.2414 - accuracy: 0.5540 - val_loss: 1.5595 - val_accuracy: 0.4672\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2317 - accuracy: 0.5552 - val_loss: 1.5637 - val_accuracy: 0.4602\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2247 - accuracy: 0.5617 - val_loss: 1.5796 - val_accuracy: 0.4610\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.2159 - accuracy: 0.5621 - val_loss: 1.5366 - val_accuracy: 0.4694\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.2096 - accuracy: 0.5663 - val_loss: 1.5270 - val_accuracy: 0.4800\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2050 - accuracy: 0.5657 - val_loss: 1.5728 - val_accuracy: 0.4608\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1971 - accuracy: 0.5674 - val_loss: 1.5736 - val_accuracy: 0.4592\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1860 - accuracy: 0.5730 - val_loss: 1.5862 - val_accuracy: 0.4572\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.1832 - accuracy: 0.5726 - val_loss: 1.5515 - val_accuracy: 0.4728\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.1725 - accuracy: 0.5796 - val_loss: 1.6234 - val_accuracy: 0.4582\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.1662 - accuracy: 0.5807 - val_loss: 1.5471 - val_accuracy: 0.4770\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.1544 - accuracy: 0.5844 - val_loss: 1.5557 - val_accuracy: 0.4766\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1535 - accuracy: 0.5867 - val_loss: 1.5859 - val_accuracy: 0.4612\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.1433 - accuracy: 0.5903 - val_loss: 1.5640 - val_accuracy: 0.4726\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.1379 - accuracy: 0.5915 - val_loss: 1.5858 - val_accuracy: 0.4632\n",
      "Epoch 57/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.1311 - accuracy: 0.5932 - val_loss: 1.5678 - val_accuracy: 0.4710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de9e53f6a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e473cd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 1.5223 - accuracy: 0.4720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.522278070449829, 0.47200000286102295]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0231f",
   "metadata": {},
   "source": [
    "## c.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6096a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 59s 31ms/step - loss: 1.8452 - accuracy: 0.3377 - val_loss: 1.6777 - val_accuracy: 0.4086\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 43s 31ms/step - loss: 1.6688 - accuracy: 0.4072 - val_loss: 1.6004 - val_accuracy: 0.4238\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.5976 - accuracy: 0.4323 - val_loss: 1.5543 - val_accuracy: 0.4350\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 43s 30ms/step - loss: 1.5480 - accuracy: 0.4474 - val_loss: 1.5040 - val_accuracy: 0.4664\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 43s 30ms/step - loss: 1.4991 - accuracy: 0.4666 - val_loss: 1.4577 - val_accuracy: 0.4784\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 44s 31ms/step - loss: 1.4638 - accuracy: 0.4787 - val_loss: 1.4232 - val_accuracy: 0.4912\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.4301 - accuracy: 0.4899 - val_loss: 1.3989 - val_accuracy: 0.4950\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.4016 - accuracy: 0.5024 - val_loss: 1.3773 - val_accuracy: 0.5052\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 44s 31ms/step - loss: 1.3770 - accuracy: 0.5130 - val_loss: 1.3733 - val_accuracy: 0.5102\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 44s 32ms/step - loss: 1.3549 - accuracy: 0.5160 - val_loss: 1.3642 - val_accuracy: 0.5130\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.3309 - accuracy: 0.5288 - val_loss: 1.3456 - val_accuracy: 0.5232\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.3108 - accuracy: 0.5374 - val_loss: 1.3769 - val_accuracy: 0.5040\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.2925 - accuracy: 0.5407 - val_loss: 1.3969 - val_accuracy: 0.4938\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 43s 30ms/step - loss: 1.2765 - accuracy: 0.5502 - val_loss: 1.3372 - val_accuracy: 0.5334\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 42s 29ms/step - loss: 1.2574 - accuracy: 0.5542 - val_loss: 1.3528 - val_accuracy: 0.5260\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.2462 - accuracy: 0.5582 - val_loss: 1.3588 - val_accuracy: 0.5204\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.2297 - accuracy: 0.5635 - val_loss: 1.3153 - val_accuracy: 0.5378\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2152 - accuracy: 0.5713 - val_loss: 1.3216 - val_accuracy: 0.5306\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.1992 - accuracy: 0.5749 - val_loss: 1.3371 - val_accuracy: 0.5216\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.1887 - accuracy: 0.5798 - val_loss: 1.3718 - val_accuracy: 0.5190\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.1712 - accuracy: 0.5868 - val_loss: 1.3507 - val_accuracy: 0.5276\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.1571 - accuracy: 0.5926 - val_loss: 1.3534 - val_accuracy: 0.5220\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.1478 - accuracy: 0.5955 - val_loss: 1.3326 - val_accuracy: 0.5314\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.1319 - accuracy: 0.6012 - val_loss: 1.3440 - val_accuracy: 0.5322\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.1238 - accuracy: 0.6027 - val_loss: 1.3617 - val_accuracy: 0.5276\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.1116 - accuracy: 0.6091 - val_loss: 1.3481 - val_accuracy: 0.5288\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.1016 - accuracy: 0.6127 - val_loss: 1.3511 - val_accuracy: 0.5332\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 43s 30ms/step - loss: 1.0927 - accuracy: 0.6152 - val_loss: 1.3309 - val_accuracy: 0.5350\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.0846 - accuracy: 0.6163 - val_loss: 1.3160 - val_accuracy: 0.5414\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.0699 - accuracy: 0.6197 - val_loss: 1.3616 - val_accuracy: 0.5308\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.0589 - accuracy: 0.6261 - val_loss: 1.3695 - val_accuracy: 0.5288\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.0483 - accuracy: 0.6301 - val_loss: 1.3823 - val_accuracy: 0.5272\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.0373 - accuracy: 0.6327 - val_loss: 1.3316 - val_accuracy: 0.5430\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 60s 42ms/step - loss: 1.0321 - accuracy: 0.6341 - val_loss: 1.3464 - val_accuracy: 0.5286 26s - loss: 1.0222 - accuracy: 0.637 - ETA: 26s - - ETA: 23s - loss: 1.0219 - accu - ETA: 17s - loss: 1.0237 - accurac - ETA: 16s - loss: \n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 43s 30ms/step - loss: 1.0198 - accuracy: 0.6395 - val_loss: 1.3322 - val_accuracy: 0.5438\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 43s 31ms/step - loss: 1.0114 - accuracy: 0.6422 - val_loss: 1.3678 - val_accuracy: 0.5386\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 0.9981 - accuracy: 0.6453 - val_loss: 1.3457 - val_accuracy: 0.5408\n",
      "157/157 [==============================] - 2s 6ms/step - loss: 1.3153 - accuracy: 0.5378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.315259575843811, 0.5378000140190125]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a8017",
   "metadata": {},
   "source": [
    "## d.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe95461",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42554042",
   "metadata": {},
   "source": [
    "## e.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effcfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ea662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
